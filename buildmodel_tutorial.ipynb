{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "\n",
    "- The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to build your own neural network.\n",
    "- Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "- A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Device for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Class\n",
    "- We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`.\n",
    "- Every `nn.Module` subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class NeuralNetwork(nn.Module):` This line defines a new class named NeuralNetwork that inherits from the nn.Module class in PyTorch. Inheriting from nn.Module allows you to create custom neural network models.\n",
    "\n",
    "`def __init__(self):` This is the initialization method (constructor) for the NeuralNetwork class. It defines the architecture of the neural network.\n",
    "\n",
    "`super().__init__():` This line calls the constructor of the parent nn.Module class, initializing the neural network as a whole.\n",
    "\n",
    "`self.flatten = nn.Flatten():` This line creates an instance of the` nn.Flatten()` module. It is used to flatten the input tensor into a 1-dimensional tensor.\n",
    "\n",
    "`self.linear_relu_stack = nn.Sequential(...):` This line defines a sequential container for the linear and activation layers of the neural network. It uses the` nn.Sequential()` module to create a sequence of layers. Inside the sequential container, the following layers are defined:\n",
    "\n",
    "- `nn.Linear(28*28, 512)`: This is the first linear layer that takes an input of size 28*28 (assuming the input is a 28x28 image) and outputs a tensor of size 512.\n",
    "- `nn.ReLU()`: This is the activation function (Rectified Linear Unit) applied after the first linear layer.\n",
    "- `nn.Linear(512, 512)`: This is the second linear layer that takes an input tensor of size 512 and outputs another tensor of size 512.\n",
    "- `nn.ReLU()`: This is the activation function applied after the second linear layer.\n",
    "- `nn.Linear(512, 10)`: This is the final linear layer that takes an input tensor of size 512 and outputs a tensor of size 10, representing the output classes.\n",
    "`def forward(self, x):` This method defines the forward pass of the neural network.\n",
    "\n",
    "`x = self.flatten()`: This line flattens the input tensor x using the flatten module defined earlier. It reshapes the input tensor from a multi-dimensional shape to a 1-dimensional shape.\n",
    "\n",
    "`logits = self.linear_relu_stack:` This line applies the sequential container linear_relu_stack to the flattened input tensor. It passes the input tensor through the defined linear and activation layers.\n",
    "\n",
    "`return logits`: This line returns the final output tensor logits from the forward pass.\n",
    "\n",
    "By defining this NeuralNetwork class, you can create an instance of the model and use it to perform forward propagation by calling the forward() method on the model object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of ``NeuralNetwork``, and move it to the ``device``, and print\n",
    "its structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model's ``forward``,\n",
    "along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n",
    "Do not call ``model.forward()`` directly!\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output.\n",
    "We get the prediction probabilities by passing it through an instance of the ``nn.Softmax`` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0330, -0.0927, -0.1446, -0.0654, -0.0040,  0.0409,  0.0790,  0.0841,\n",
      "         -0.0793, -0.0017]], grad_fn=<AddmmBackward0>)\n",
      "Predicted class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device = device)\n",
    "logits = model(X)\n",
    "print(logits)\n",
    "pred_probab = nn.Softmax(dim = 1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X = torch.rand(1, 28, 28, device=device):` This line creates a random tensor X with a shape of (1, 28, 28) on the specified device. It represents a single input sample, assumed to be a 28x28 image.\n",
    "\n",
    "`logits = model(X):` This line passes the input tensor X through the model, invoking the forward() method of the model. It computes the logits (raw output) of the model for the given input.\n",
    "\n",
    "`pred_probab = nn.Softmax(dim=1)(logits):` This line applies the softmax function along dimension 1 to the logits tensor using nn.Softmax(dim=1). It converts the logits into predicted probabilities for each class.\n",
    "\n",
    "`y_pred = pred_probab.argmax(1):` This line finds the index of the class with the highest probability by calling argmax(1) on pred_probab. It returns a tensor y_pred containing the predicted class label.\n",
    "\n",
    "By executing this code, you can obtain the predicted class label for the given input tensor X using the trained NeuralNetwork model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
    "layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\n",
    "the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "is a module that applies a linear transformation on the input using its stored weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features = 28*28, out_features=20)\n",
    "hidden1 = layer(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ReLU\n",
    "- Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
    "- They are applied after linear transformations to introduce *nonlinearity*, helping neural networks learn a wide variety of phenomena.\n",
    "\n",
    "In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there's other activations to introduce non-linearity in your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-2.8053e-01,  1.5342e-01,  2.7263e-01,  1.0547e-01, -4.2795e-01,\n",
      "         -1.4632e-01,  5.4631e-01, -4.8849e-01,  5.0240e-01, -2.9074e-02,\n",
      "          1.9351e-01, -4.6250e-01,  1.2593e-01,  7.3368e-01, -2.0106e-01,\n",
      "         -3.7836e-04, -3.9227e-01,  3.2677e-02,  1.3237e-01,  6.7212e-01],\n",
      "        [-1.7836e-01, -4.1733e-02,  4.7869e-01,  6.5184e-02, -3.4521e-01,\n",
      "         -1.4592e-01,  4.8984e-01, -9.4041e-01,  4.7825e-01,  2.1122e-02,\n",
      "          1.4657e-01, -6.8349e-01,  8.3621e-02,  3.4088e-01, -3.0523e-01,\n",
      "          1.8552e-01, -4.2274e-01, -3.6072e-01,  2.7483e-02,  9.4940e-02],\n",
      "        [-4.1318e-01,  1.8033e-01,  3.1851e-01, -1.9846e-01,  8.7675e-02,\n",
      "         -5.8274e-02,  3.4326e-01, -6.3833e-01,  2.7314e-01, -1.0070e-01,\n",
      "         -1.6563e-01, -5.2347e-01, -1.4071e-01,  4.1281e-01, -2.1325e-01,\n",
      "          5.5213e-02, -3.9994e-01, -3.1361e-01,  3.7067e-01,  4.8612e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.1534, 0.2726, 0.1055, 0.0000, 0.0000, 0.5463, 0.0000, 0.5024,\n",
      "         0.0000, 0.1935, 0.0000, 0.1259, 0.7337, 0.0000, 0.0000, 0.0000, 0.0327,\n",
      "         0.1324, 0.6721],\n",
      "        [0.0000, 0.0000, 0.4787, 0.0652, 0.0000, 0.0000, 0.4898, 0.0000, 0.4782,\n",
      "         0.0211, 0.1466, 0.0000, 0.0836, 0.3409, 0.0000, 0.1855, 0.0000, 0.0000,\n",
      "         0.0275, 0.0949],\n",
      "        [0.0000, 0.1803, 0.3185, 0.0000, 0.0877, 0.0000, 0.3433, 0.0000, 0.2731,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4128, 0.0000, 0.0552, 0.0000, 0.0000,\n",
      "         0.3707, 0.0486]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered\n",
    "container of modules. The data is passed through all the modules in the same order as defined. You can use\n",
    "sequential containers to put together a quick network like ``seq_modules``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Softmax\n",
    "The last linear layer of the neural network returns `logits` - raw values in [-\\infty, \\infty] - which are passed to the\n",
    "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values\n",
    "[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n",
    "which the values must sum to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1046, 0.0923, 0.0876, 0.0948, 0.1008, 0.1055, 0.1096, 0.1101, 0.0935,\n",
       "         0.1011]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "Many layers inside a neural network are *parameterized*, i.e. have associated weights\n",
    "and biases that are optimized during training. Subclassing ``nn.Module`` automatically\n",
    "tracks all fields defined inside your model object, and makes all parameters\n",
    "accessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values?: tensor([[ 0.0077, -0.0142, -0.0143,  ...,  0.0182, -0.0334, -0.0028],\n",
      "        [ 0.0063,  0.0172,  0.0037,  ..., -0.0197,  0.0016,  0.0213]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values?: tensor([-0.0096,  0.0110], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values?: tensor([[-0.0147, -0.0327, -0.0097,  ...,  0.0046,  0.0188,  0.0307],\n",
      "        [-0.0155,  0.0263, -0.0262,  ...,  0.0319, -0.0217,  0.0427]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values?: tensor([0.0227, 0.0392], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values?: tensor([[ 0.0246,  0.0004, -0.0045,  ...,  0.0015, -0.0202,  0.0320],\n",
      "        [-0.0369, -0.0278, -0.0009,  ...,  0.0381, -0.0037, -0.0140]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values?: tensor([-0.0097,  0.0345], grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values?: {param[:2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for name, param in model.named_parameters():`: This line initiates a loop that iterates over the named parameters of the model. The `named_parameters()` method returns an iterator over module parameters, yielding both the name of the parameter and the parameter itself.\n",
    "\n",
    "`print(f\"Layer: {name} | Size: {param.size()} | Values?: {param[:2]}\\n\"):` This line prints the details of each parameter. Within the loop, it does the following:\n",
    "\n",
    "- `name` represents the name of the current parameter (e.g., \"linear_relu_stack.0.weight\").\n",
    "- `param.size()` prints the size (shape) of the current parameter tensor.\n",
    "- `param[:2]` prints the first two elements of the parameter tensor as an example. This provides a glimpse of the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
